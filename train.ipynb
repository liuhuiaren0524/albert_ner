{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM/jbZQXYyh8u/CpafdNXlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuhuiaren0524/albert_ner/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCzWNk7f3sAU"
      },
      "source": [
        "from albert_ner import model, tokenizer, NER, evaluate\n",
        "from albert_ner import maxlen, id2label, label2id, num_labels\n",
        "maxlen = 256\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "learing_rate = 1e-5  # bert_layers越小，学习率应该要越大\n",
        "crf_lr_multiplier = 1000  # 必要时扩大CRF层的学习率\n",
        "\n",
        "def load_data(filename):\n",
        "    D = []\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        f = f.read()\n",
        "        for l in f.split('\\n\\n'):\n",
        "            if not l:\n",
        "                continue\n",
        "            d, last_flag = [], ''\n",
        "            for c in l.split('\\n'):\n",
        "                char, this_flag = c.split('\\t')\n",
        "                this_flag = this_flag.upper()\n",
        "                if this_flag != 'O':\n",
        "                    this_flag += '-CAR'\n",
        "                if this_flag == 'O' and last_flag == 'O':\n",
        "                    d[-1][0] += char\n",
        "                elif this_flag == 'O' and last_flag != 'O':\n",
        "                    d.append([char, 'O'])\n",
        "                elif this_flag[:1] == 'B':\n",
        "                    d.append([char, this_flag[2:]])\n",
        "\n",
        "                else:\n",
        "                    d[-1][0] += char\n",
        "                last_flag = this_flag\n",
        "            D.append(d)\n",
        "    return D\n",
        "\n",
        "\n",
        "# 标注数据\n",
        "train_data = load_data('/home/CARNER/dataset/train_data.txt')\n",
        "valid_data = load_data('/home//CARNER/dataset/valid_data.txt')\n",
        "test_data = load_data('/home/CARNER/dataset/test_data.txt')\n",
        "\n",
        "with open('/home/CARNER/dataset/veco_series_list.txt', 'r', encoding='utf-8') as f:\n",
        "    veco_series_list = [row.strip() for row in f if row.strip()]\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, enhance=False, frac=0.2, **kw):\n",
        "        super().__init__(*args, **kw)\n",
        "        self.enhance = enhance\n",
        "        self.frac = frac\n",
        "\n",
        "    def __iter__(self, random_=False):\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "        for is_end, item in self.sample(random_):\n",
        "            token_ids, labels = [tokenizer._token_start_id], [0]\n",
        "            for w, l in item:\n",
        "                if self.enhance and (l != 'O') and (random.random() < self.frac):\n",
        "                    w = random.choice(veco_series_list)\n",
        "                w_token_ids = tokenizer.encode(w)[0][1:-1]\n",
        "                if len(token_ids) + len(w_token_ids) < maxlen:\n",
        "                    token_ids += w_token_ids\n",
        "                    if l == 'O':\n",
        "                        labels += [0] * len(w_token_ids)\n",
        "                    else:\n",
        "                        B = label2id[l] * 2 + 1\n",
        "                        I = label2id[l] * 2 + 2\n",
        "                        labels += ([B] + [I] * (len(w_token_ids) - 1))\n",
        "                else:\n",
        "                    break\n",
        "            token_ids += [tokenizer._token_end_id]\n",
        "            labels += [0]\n",
        "            segment_ids = [0] * len(token_ids)\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_labels.append(labels)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_labels = sequence_padding(batch_labels)\n",
        "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_val_f1 = 0\n",
        "val_history, test_history = [], []\n",
        "class Evaluator(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.best_val_f1 = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        global best_val_f1, val_history, test_history\n",
        "        trans = K.eval(CRF.trans)\n",
        "        NER.trans = trans\n",
        "        print(NER.trans)\n",
        "        print('Valid:')\n",
        "        f1, precision, recall = evaluate(valid_data, location=True)\n",
        "        val_history.append([f1, precision, recall])\n",
        "        # 保存最优\n",
        "        if f1 >= best_val_f1:\n",
        "            best_val_f1 = f1\n",
        "            model.save_weights('/home/CARNER/checkpoints/best_model.weights')\n",
        "        print('f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' % (f1, precision, recall, best_val_f1))\n",
        "        print('Test:')\n",
        "        f1, precision, recall = evaluate(test_data, location=True)\n",
        "        test_history.append([f1, precision, recall])\n",
        "        print('f1: %.5f, precision: %.5f, recall: %.5f\\n' % (f1, precision, recall))\n",
        "\n",
        "        \n",
        "# model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss=CRF.sparse_loss,\n",
        "    optimizer=Adam(learing_rate),\n",
        "    metrics=[CRF.sparse_accuracy]\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    evaluator = Evaluator()\n",
        "    print('*'*20)\n",
        "    print('*'*20)\n",
        "    print('Step 1:')\n",
        "    epochs = 3\n",
        "    best_frac = 0\n",
        "    tmp_val_f1 = 0\n",
        "    for frac in [0, 0.1, 0.2, 0.3]:\n",
        "        train_generator = data_generator(train_data, batch_size, enhance=True, frac=frac)\n",
        "        model.fit_generator(\n",
        "            train_generator.forfit(),\n",
        "            steps_per_epoch=len(train_generator),\n",
        "            epochs=epochs,\n",
        "            callbacks=[evaluator]\n",
        "        )\n",
        "        if best_val_f1 > tmp_val_f1:\n",
        "            tmp_val_f1 = best_val_f1\n",
        "            best_frac = frac\n",
        "\n",
        "    print('*'*20)\n",
        "    print('*'*20)\n",
        "    print('Step 2:')\n",
        "    best_frac = 0.4\n",
        "    print('Best random Frac:', best_frac)\n",
        "    epochs = 20\n",
        "    train_generator = data_generator(train_data, batch_size, enhance=True, frac=best_frac)\n",
        "    model.fit_generator(\n",
        "        train_generator.forfit(),\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=epochs,\n",
        "        callbacks=[evaluator]\n",
        "    )\n",
        "    \n",
        "    with open('valid_history.txt', 'w') as f:\n",
        "        f.write('\\n'.join(['{}\\t{}\\t{}'.format(*row) for row in val_history]))\n",
        "\n",
        "    with open('test_history.txt', 'w') as f:\n",
        "        f.write('\\n'.join(['{}\\t{}\\t{}'.format(*row) for row in test_history]))\n",
        "\n",
        "    corpus = [\"而另一款捷达则排名第12位,这款1.4l的捷达并不是其最畅销的一款,毕竟作为家轿,1.4l的排量还是不太够的。\",\n",
        "              \"新宝来和新速腾的发动机,如果两款车同为1.6l排量,或许同为1.4t排量的话,两款车的发动机都是相同的。\",\n",
        "              \"而朗逸销量最好的一款是排名第35位的1.6l手自一体车型,它的百公里油耗和捷达一样都是7.19l。\",\n",
        "              \"依旧是五座布局,至于明年国产后,会不会再如现款一样推出2+3+2的七座车型,目前还没有官方消息,不过,为了抢占市场,以及与本田cr-v和rav4荣放形成差异化,相信大概率国产新奇骏还是会推出七座车型的。\",\n",
        "              \"从销量上来看,比亚迪宋的增幅是十强榜单的第一,今年1-6月合计销售了77340辆,相比去年,同比增长高达331.9%,要说比亚迪宋的大爆发,就离不开车系里最热门的车型宋pro,从整车的设计来看,高颜值和高科技配置都非常亮眼,另外价格方面也是比亚迪宋非常热销的原因,8.98-11.98万的价格区间非常亲民,让拥有越级配置的宋pro更具性价比。\"\n",
        "              ]\n",
        "\n",
        "    for text in corpus:\n",
        "        print(NER.recognize(text, location=True))\n",
        "# else:\n",
        "#     model.load_weights('./best_model.weights')\n",
        "    # print('Test:')\n",
        "    # f1, precision, recall = evaluate(test_data)\n",
        "    # print('f1: %.5f, precision: %.5f, recall: %.5f\\n' % (f1, precision, recall))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}